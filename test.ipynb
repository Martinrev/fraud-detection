{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4646773 entries, 0 to 4646773\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   FlagImpaye       category      \n",
      " 1   Montant          float64       \n",
      " 2   DateTransaction  datetime64[ns]\n",
      " 3   CodeDecision     object        \n",
      " 4   VerifianceCPT1   int32         \n",
      " 5   VerifianceCPT2   int32         \n",
      " 6   VerifianceCPT3   int32         \n",
      " 7   D2CB             int32         \n",
      " 8   ScoringFP1       float64       \n",
      " 9   ScoringFP2       float64       \n",
      " 10  ScoringFP3       float64       \n",
      " 11  TauxImpNb_RB     float64       \n",
      " 12  TauxImpNB_CPM    float64       \n",
      " 13  EcartNumCheq     int32         \n",
      " 14  NbrMagasin3J     int32         \n",
      " 15  DiffDateTr1      float64       \n",
      " 16  DiffDateTr2      float64       \n",
      " 17  DiffDateTr3      float64       \n",
      " 18  CA3TRetMtt       float64       \n",
      " 19  CA3TR            float64       \n",
      " 20  Heure            int32         \n",
      " 21  Year             int32         \n",
      " 22  Month            int32         \n",
      " 23  Day              int32         \n",
      " 24  Hour             int32         \n",
      " 25  Minute           int32         \n",
      " 26  Second           int32         \n",
      "dtypes: category(1), datetime64[ns](1), float64(11), int32(13), object(1)\n",
      "memory usage: 731.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On définit la partie Train et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dates limites pour les ensembles d'apprentissage et de test\n",
    "train_inf = '2017-02-01'\n",
    "train_sup = '2017-08-31'\n",
    "test_inf = '2017-09-01'\n",
    "test_sup = '2017-11-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[(df['DateTransaction'] >= train_inf) & (df['DateTransaction'] <= train_sup)]\n",
    "X_train = train.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_train = train['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[(df['DateTransaction'] >= test_inf) & (df['DateTransaction'] <= test_sup)]\n",
    "X_test = test.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_test = test['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888468"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737068"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de chaque méthode d'échantillonage par modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'imblearn'\"\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install 'imblearn'\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\python311\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from xgboost) (1.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait tourner DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier, SVC, adaboost, xgboost et LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress: 100%|██████████| 4/4 [02:52<00:00, 43.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# On définit les stratégies d'échantillonnage\n",
    "sampling_strategies = [0.05, 0.1]  \n",
    "\n",
    "#On garde deux oversampling et deux undersampling\n",
    "samplers = [RandomOverSampler, SMOTE, RandomUnderSampler, NearMiss]\n",
    "\n",
    "# Précalcule les échantillons resamplés pour chaque méthode d'échantillonnage\n",
    "resampled_data = {}\n",
    "for sampler in tqdm(samplers, desc=\"Resampling Progress\"):\n",
    "    for strategy in tqdm(sampling_strategies, desc=f\"{sampler.__name__} Progress\", leave=False):\n",
    "        key = (sampler, strategy)\n",
    "        X_resampled, y_resampled = sampler(sampling_strategy=strategy).fit_resample(X_train, y_train)\n",
    "        resampled_data[key] = (X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marevel\\AppData\\Local\\Temp\\ipykernel_14404\\260053566.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_1 = pd.concat([results_df_1, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.030195815287012748\n",
      "Model: DecisionTreeClassifier, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.026262189853104554\n",
      "Model: DecisionTreeClassifier, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.03714936856354632\n",
      "Model: DecisionTreeClassifier, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.03639382151402204\n",
      "Model: DecisionTreeClassifier, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.03486636000379733\n",
      "Model: DecisionTreeClassifier, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.03289138545840045\n",
      "Model: DecisionTreeClassifier, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.02929911735260891\n",
      "Model: DecisionTreeClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.025979530864366336\n",
      "                    Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  DecisionTreeClassifier   RandomOverSampler               0.05  0.030196\n",
      "1  DecisionTreeClassifier   RandomOverSampler               0.10  0.026262\n",
      "2  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
      "3  DecisionTreeClassifier               SMOTE               0.10  0.036394\n",
      "4  DecisionTreeClassifier  RandomUnderSampler               0.05  0.034866\n",
      "5  DecisionTreeClassifier  RandomUnderSampler               0.10  0.032891\n",
      "6  DecisionTreeClassifier            NearMiss               0.05  0.029299\n",
      "7  DecisionTreeClassifier            NearMiss               0.10  0.025980\n"
     ]
    }
   ],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_1 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # On parcourt chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # On parcourt chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupére les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialise et entraîne le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évalue les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajoute les résultats au DataFrame\n",
    "            results_df_1 = pd.concat([results_df_1, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Affiche le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_1)\n",
    "results_df_1.to_pickle('dataframe_recap_sampling_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.02952237759137063\n",
      "                    Model   Sampler  Sampling Strategy  F1 Score\n",
      "0  RandomForestClassifier  NearMiss                0.1  0.029522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marevel\\AppData\\Local\\Temp\\ipykernel_14404\\4242767768.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_2_bis = pd.concat([results_df_2_bis, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# On définit le modèle à tester\n",
    "models = [\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_2 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # On parcourt chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # On parcourt chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupére les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialise et entraîne le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Fait des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évalue les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajoute les résultats au DataFrame\n",
    "            results_df_2 = pd.concat([results_df_2, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Affiche le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_2)\n",
    "results_df_2.to_pickle('dataframe_recap_sampling_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_3 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # On parcourt chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # On parcourt chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupére les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialise et entraîne le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Fait des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évalue les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajoute les résultats au DataFrame\n",
    "            results_df_3 = pd.concat([results_df_3, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                \n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Affiche le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_3)\n",
    "results_df_3.to_pickle('dataframe_recap_sampling_3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle AdaBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_4 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # On parcourt chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # On parcourt chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupére les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialise et entraîne le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Fait des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évalue les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajoute les résultats au DataFrame\n",
    "            results_df_4 = pd.concat([results_df_4, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Affiche le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_4)\n",
    "results_df_4.to_pickle('dataframe_recap_sampling_4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle XGBClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_5 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # On parcourt chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # On parcourt chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupére les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialise et entraîne le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Fait des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évalue les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajoute les résultats au DataFrame\n",
    "            results_df_5 = pd.concat([results_df_5, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Affiche le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_5)\n",
    "results_df_5.to_pickle('dataframe_recap_sampling_5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# On définit le modèle à tester\n",
    "models = [\n",
    "    LogisticRegression(max_iter= 1000) \n",
    "]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_6 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # On parcourt chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # On parcourt chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupére les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialise et entraîne le modèle de régression logistique\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Fait des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évalue les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajoute les résultats au DataFrame\n",
    "            results_df_6 = pd.concat([results_df_6, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Affiche le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_6)\n",
    "results_df_6.to_pickle('dataframe_recap_sampling_6_1000.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réunit les résultats dans un seul tableau recapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_1 = pd.read_pickle('dataframe_recap_sampling_1.pkl')\n",
    "results_df_2 = pd.read_pickle('dataframe_recap_sampling_2.pkl')\n",
    "results_df_4 = pd.read_pickle('dataframe_recap_sampling_4.pkl')\n",
    "results_df_5 = pd.read_pickle('dataframe_recap_sampling_5.pkl')\n",
    "results_df_6 = pd.read_pickle('dataframe_recap_sampling_6_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df_1, results_df_2, results_df_4, results_df_5, results_df_6], ignore_index=True)\n",
    "results_df.to_pickle('dataframe_recap_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.056036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.032969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.015576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.031354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.038366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.072764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model            Sampler  Sampling Strategy  \\\n",
       "count                       32                 32            32.0000   \n",
       "unique                       4                  4                NaN   \n",
       "top     DecisionTreeClassifier  RandomOverSampler                NaN   \n",
       "freq                         8                  8                NaN   \n",
       "mean                       NaN                NaN             0.0750   \n",
       "std                        NaN                NaN             0.0254   \n",
       "min                        NaN                NaN             0.0500   \n",
       "25%                        NaN                NaN             0.0500   \n",
       "50%                        NaN                NaN             0.0750   \n",
       "75%                        NaN                NaN             0.1000   \n",
       "max                        NaN                NaN             0.1000   \n",
       "\n",
       "         F1 Score  \n",
       "count   32.000000  \n",
       "unique        NaN  \n",
       "top           NaN  \n",
       "freq          NaN  \n",
       "mean     0.056036  \n",
       "std      0.032969  \n",
       "min      0.015576  \n",
       "25%      0.031354  \n",
       "50%      0.038366  \n",
       "75%      0.072764  \n",
       "max      0.131428  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant faire la même chose mais cette fois-ci sans les données rééchantilonnées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [DecisionTreeClassifier()]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_no_sampling = pd.DataFrame(columns=['Model', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # Entraîne le modèle sur les données d'entraînement d'origine\n",
    "    model.fit(X_train, y_train)\n",
    "            \n",
    "    # Fait des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évalue les performances du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Ajoute les résultats au DataFrame\n",
    "    results_df_no_sampling = pd.concat([results_df_no_sampling, pd.DataFrame({\n",
    "        'Model': [model.__class__.__name__],\n",
    "        'F1 Score': [f1]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Affiche le message de progression\n",
    "    tqdm.write(f\"Model: {model.__class__.__name__}, F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_no_sampling)\n",
    "results_df_no_sampling.to_pickle('dataframe_recap_no_sampling_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [RandomForestClassifier()]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_no_sampling_2 = pd.DataFrame(columns=['Model', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # Entraîne le modèle sur les données d'entraînement d'origine\n",
    "    model.fit(X_train, y_train)\n",
    "            \n",
    "    # Fait des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évalue les performances du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Ajoute les résultats au DataFrame\n",
    "    results_df_no_sampling_2 = pd.concat([results_df_no_sampling_2, pd.DataFrame({\n",
    "        'Model': [model.__class__.__name__],\n",
    "        'F1 Score': [f1]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Affiche le message de progression\n",
    "    tqdm.write(f\"Model: {model.__class__.__name__}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_no_sampling_2)\n",
    "results_df_no_sampling_2.to_pickle('dataframe_recap_no_sampling_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [SVC()]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_no_sampling_3 = pd.DataFrame(columns=['Model', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # Entraîne le modèle sur les données d'entraînement d'origine\n",
    "    model.fit(X_train, y_train)\n",
    "            \n",
    "    # Fait des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évalue les performances du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Ajoute les résultats au DataFrame\n",
    "    results_df_no_sampling_3 = pd.concat([results_df_no_sampling_3, pd.DataFrame({\n",
    "        'Model': [model.__class__.__name__],\n",
    "        'F1 Score': [f1]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Affiche le message de progression\n",
    "    tqdm.write(f\"Model: {model.__class__.__name__}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_no_sampling_3)\n",
    "results_df_no_sampling_3.to_pickle('dataframe_recap_no_sampling_3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle AdaBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [AdaBoostClassifier()]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_no_sampling_4 = pd.DataFrame(columns=['Model', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # Entraîne le modèle sur les données d'entraînement d'origine\n",
    "    model.fit(X_train, y_train)\n",
    "            \n",
    "    # Fait des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évalue les performances du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Ajoute les résultats au DataFrame\n",
    "    results_df_no_sampling_4 = pd.concat([results_df_no_sampling_4, pd.DataFrame({\n",
    "        'Model': [model.__class__.__name__],\n",
    "        'F1 Score': [f1]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Affiche le message de progression\n",
    "    tqdm.write(f\"Model: {model.__class__.__name__}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_no_sampling_4)\n",
    "results_df_no_sampling_4.to_pickle('dataframe_recap_no_sampling_4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle XGBClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [XGBClassifier()]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_no_sampling_5 = pd.DataFrame(columns=['Model', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # Entraîne le modèle sur les données d'entraînement d'origine\n",
    "    model.fit(X_train, y_train)\n",
    "            \n",
    "    # Fait des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évalue les performances du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Ajoute les résultats au DataFrame\n",
    "    results_df_no_sampling_5 = pd.concat([results_df_no_sampling_5, pd.DataFrame({\n",
    "        'Model': [model.__class__.__name__],\n",
    "        'F1 Score': [f1]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Affiche le message de progression\n",
    "    tqdm.write(f\"Model: {model.__class__.__name__}, F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_no_sampling_5)\n",
    "results_df_no_sampling_5.to_pickle('dataframe_recap_no_sampling_5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle à tester\n",
    "models = [LogisticRegression()]\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les résultats\n",
    "results_df_no_sampling_6 = pd.DataFrame(columns=['Model','F1 Score'])\n",
    "\n",
    "# On parcourt chaque modèle\n",
    "for model in models:\n",
    "    # Entraîne le modèle sur les données d'entraînement d'origine\n",
    "    model.fit(X_train, y_train)\n",
    "            \n",
    "    # Fait des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Évalue les performances du modèle\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Ajoute les résultats au DataFrame\n",
    "    results_df_no_sampling_6 = pd.concat([results_df_no_sampling_6, pd.DataFrame({\n",
    "        'Model': [model.__class__.__name__],\n",
    "        'F1 Score': [f1]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Affiche le message de progression\n",
    "    tqdm.write(f\"Model: {model.__class__.__name__},  F1 Score: {f1}\")\n",
    "\n",
    "# Affiche le tableau récapitulatif\n",
    "print(results_df_no_sampling_6)\n",
    "results_df_no_sampling_6.to_pickle('dataframe_recap_no_sampling_6.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réunit les résultats sans rééchantillonage dans un seul tableau récapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_no_sampling_1 = pd.read_pickle('dataframe_recap_no_sampling.pkl')\n",
    "results_df_no_sampling_2 = pd.read_pickle('dataframe_recap_no_sampling_2.pkl')\n",
    "results_df_no_sampling_4 = pd.read_pickle('dataframe_recap_no_sampling_4.pkl')\n",
    "results_df_no_sampling_5 = pd.read_pickle('dataframe_recap_no_sampling_5.pkl')\n",
    "results_df_no_sampling_6 = pd.read_pickle('dataframe_recap_no_sampling_6.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_no_sampling = pd.concat([results_df_no_sampling_1, results_df_no_sampling_2, results_df_no_sampling_4, results_df_no_sampling_5, results_df_no_sampling_6], ignore_index=True)\n",
    "results_df_no_sampling.to_pickle('dataframe_recap_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_no_sampling.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rassemble nos deux tableaux récapitulatifs pour avoir tous nos résultats dans un seul tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sampling = pd.read_pickle('dataframe_recap_sampling1.pkl')\n",
    "result_no_sampling = pd.read_pickle('dataframe_recap_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([result_sampling, result_no_sampling])\n",
    "results_df.to_pickle('dataframe_recap_sampling_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec notre tableau récapitulatif global, nous allons maintenant déterminer les meilleurs modèles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model             Sampler  Sampling Strategy  F1 Score\n",
      "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
      "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
      "2  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
      "3           XGBClassifier               SMOTE               0.10  0.077850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marevel\\AppData\\Local\\Temp\\ipykernel_16984\\346220120.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  best_results_df = pd.concat([best_results_df, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "# On groupe les résultats par modèle\n",
    "grouped_results = results_df.groupby('Model')\n",
    "\n",
    "# Initialisation d'un DataFrame pour stocker les meilleurs résultats par modèle\n",
    "best_results_df = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# On parcourt chaque groupe\n",
    "for model, group_df in grouped_results:\n",
    "    # Trouve la ligne avec le plus grand F1 score pour chaque modèle\n",
    "    best_result = group_df.sort_values(by='F1 Score', ascending=False).iloc[0]\n",
    "    \n",
    "    # Ajoute le meilleur résultat au DataFrame final\n",
    "    # Remplace la ligne best_results_df = best_results_df.append(...) par ceci\n",
    "    best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "        'Model': [best_result['Model']],\n",
    "        'Sampler': [best_result['Sampler']],\n",
    "        'Sampling Strategy': [best_result['Sampling Strategy']],\n",
    "        'F1 Score': [best_result['F1 Score']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Affiche le tableau récapitulatif des meilleurs résultats pour chaque modèle\n",
    "print(best_results_df)\n",
    "best_results_df.to_pickle('best_results_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.037149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomUnderSampler</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.102525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model             Sampler  Sampling Strategy  F1 Score\n",
       "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
       "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
       "2  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
       "3           XGBClassifier               SMOTE               0.10  0.077850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant qu'on a une vue d'ensemble claire sur les méthodes de re-échantillonage qui marchent le mieux en fonction des modèles, on va reprendre ces combinaisons pour réaliser une optimisation cette fois-ci des hyper-paramètres des modèles à l'aide d'un greedsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle AdaBoostClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'AdaBoostClassifier\n",
    "adab_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())  # Utilise un arbre de décision faible par défaut\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', adab_classifier)\n",
    "])\n",
    "\n",
    "# Crée un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__base_estimator__max_depth': [2, 3],  # Ajustez ici pour le paramètre de l'arbre faible\n",
    "    'classifier__learning_rate': [0.1, 0.5],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "\n",
    "# Barre de progression pour grid search\n",
    "for train_index, test_index in tqdm(tscv.split(X_train, y_train), desc=\"Grid Search Progress\"):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "# Affiche les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_adaboost.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.05)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('classifier', rf_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, scoring='f1', cv=tscv)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Affiche les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (Random Forest):\", grid_search_rf.best_params_)\n",
    "print(\"Meilleur score F1 (Random Forest):\", grid_search_rf.best_score_)\n",
    "\n",
    "with open('modele_random_forest.pkl', 'wb') as file_rf:\n",
    "    pickle.dump(grid_search_rf, file_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle XGBClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres (XGBoost): {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Meilleur score F1 (XGBoost): 0.09887753490587534\n",
      "Score moyen: 0.029262497240361184, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.028306750383856456, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.05709189714118945, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.05738477767642375, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.07783827808542546, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.07770718220803052, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.09887753490587534, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.09715042768788644, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.006933445776624136, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.006374327643309981, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.018093108658124028, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.021169163727159133, Paramètres: {'classifier__colsample_bytree': 0.8, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.02925869984034272, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.02662186932263279, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.05692660576873543, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.055776218082478554, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.07171206913250401, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.07052731866043352, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.09571501387761969, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.09416571423285273, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.0074406505551835615, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.006880960497486812, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.0, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 50, 'classifier__subsample': 1.0}\n",
      "Score moyen: 0.02140463408356864, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Score moyen: 0.02345281956651603, Paramètres: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 100, 'classifier__subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du XGBClassifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('classifier', xgb_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [3, 7],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__learning_rate': [0.1, 0.01],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, scoring='f1', cv=tscv)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Affiche les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (XGBoost):\", grid_search_xgb.best_params_)\n",
    "print(\"Meilleur score F1 (XGBoost):\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Affiche les résultats détaillés de la recherche sur la grille\n",
    "results = grid_search_xgb.cv_results_\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    print(f\"Score moyen: {mean_score}, Paramètres: {params}\")\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_xgboost.pkl', 'wb') as file_xgb:\n",
    "    pickle.dump(grid_search_xgb, file_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle GradientBoostingClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres: {'classifier__max_depth': 15, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n",
      "Meilleur score F1: 0.10323029387708163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.1)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Crée un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    " \n",
    "# Affiche les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_gboost.pkl', 'wb') as file_gb:\n",
    "    pickle.dump(gb_classifier, file_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle GradientBoostingClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.1)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Crée un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train_ros, y_train_ros)\n",
    "\n",
    " \n",
    "# Affiche les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 0: 2290 occurrences\n",
      "Classe 1: 229 occurrences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compte le nombre d'éléments uniques dans y_train_rus\n",
    "unique_classes, counts = np.unique(y_train_rus, return_counts=True)\n",
    "\n",
    "# Affiche le nombre d'éléments uniques et leurs occurrences\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Classe {cls}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38827"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score sur l'ensemble de test: 0.054590570719602965\n"
     ]
    }
   ],
   "source": [
    "# Utilise le meilleur modèle trouvé par la recherche sur la grille\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fait des prédictions sur l'ensemble de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcule le F1-score sur l'ensemble de test\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "# Affichage du F1-score sur l'ensemble de test\n",
    "print(\"F1 Score sur l'ensemble de test:\", f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
