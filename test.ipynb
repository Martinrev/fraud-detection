{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On définit la partie Train et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dates limites pour les ensembles d'apprentissage et de test\n",
    "train_inf = '2017-02-01'\n",
    "train_sup = '2017-08-31'\n",
    "test_inf = '2017-09-01'\n",
    "test_sup = '2017-11-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[(df['DateTransaction'] >= train_inf) & (df['DateTransaction'] <= train_sup)]\n",
    "X_train = train.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_train = train['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[(df['DateTransaction'] >= test_inf) & (df['DateTransaction'] <= test_sup)]\n",
    "X_test = test.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_test = test['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888468"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de chaque méthode d'échantillonage par modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'imblearn'\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\python311\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from xgboost) (1.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait tourner DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier,KNeighborsClassifier, SVC, adaboost et xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress: 100%|██████████| 4/4 [14:03<00:00, 210.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Définit les stratégies d'échantillonnage\n",
    "sampling_strategies = [0.05, 0.1]  \n",
    "#on garde deux oversampling et deux undersampling\n",
    "samplers = [RandomOverSampler, SMOTE, RandomUnderSampler, NearMiss]\n",
    "\n",
    "# Précalcule les échantillons resamplés pour chaque méthode d'échantillonnage\n",
    "resampled_data = {}\n",
    "for sampler in tqdm(samplers, desc=\"Resampling Progress\"):\n",
    "    for strategy in tqdm(sampling_strategies, desc=f\"{sampler.__name__} Progress\", leave=False):\n",
    "        key = (sampler, strategy)\n",
    "        X_resampled, y_resampled = sampler(sampling_strategy=strategy).fit_resample(X_train, y_train)\n",
    "        resampled_data[key] = (X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_1 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_1 = pd.concat([results_df_1, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_1)\n",
    "results_df_1.to_pickle('dataframe_recap_sampling_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_2 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_2 = pd.concat([results_df_2, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_2)\n",
    "results_df_2.to_pickle('dataframe_recap_sampling_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_3 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_3 = pd.concat([results_df_3, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_3)\n",
    "results_df_3.to_pickle('dataframe_recap_sampling_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4044\\3750963144.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_4 = pd.concat([results_df_4, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.11692036179130819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.13142780820088154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.015575857420997454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.022629149918118208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.10746336476201294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.12748484650980904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.03958164977932492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.03174043378592841\n",
      "                Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  AdaBoostClassifier   RandomOverSampler               0.05  0.116920\n",
      "1  AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
      "2  AdaBoostClassifier               SMOTE               0.05  0.015576\n",
      "3  AdaBoostClassifier               SMOTE               0.10  0.022629\n",
      "4  AdaBoostClassifier  RandomUnderSampler               0.05  0.107463\n",
      "5  AdaBoostClassifier  RandomUnderSampler               0.10  0.127485\n",
      "6  AdaBoostClassifier            NearMiss               0.05  0.039582\n",
      "7  AdaBoostClassifier            NearMiss               0.10  0.031740\n"
     ]
    }
   ],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_4 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_4 = pd.concat([results_df_4, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_4)\n",
    "results_df_4.to_pickle('dataframe_recap_sampling_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4044\\3536015111.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_5 = pd.concat([results_df_5, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBClassifier, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.0715716151501959\n",
      "Model: XGBClassifier, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.058997050147492625\n",
      "Model: XGBClassifier, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.06164853474468294\n",
      "Model: XGBClassifier, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.07784960871044573\n",
      "Model: XGBClassifier, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.07633948494474051\n",
      "Model: XGBClassifier, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.0656522313176657\n",
      "Model: XGBClassifier, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.03485663082437276\n",
      "Model: XGBClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.029324231670471584\n",
      "           Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  XGBClassifier   RandomOverSampler               0.05  0.071572\n",
      "1  XGBClassifier   RandomOverSampler               0.10  0.058997\n",
      "2  XGBClassifier               SMOTE               0.05  0.061649\n",
      "3  XGBClassifier               SMOTE               0.10  0.077850\n",
      "4  XGBClassifier  RandomUnderSampler               0.05  0.076339\n",
      "5  XGBClassifier  RandomUnderSampler               0.10  0.065652\n",
      "6  XGBClassifier            NearMiss               0.05  0.034857\n",
      "7  XGBClassifier            NearMiss               0.10  0.029324\n"
     ]
    }
   ],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_5 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_5 = pd.concat([results_df_5, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_5)\n",
    "results_df_5.to_pickle('dataframe_recap_sampling_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24944\\664018385.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_6 = pd.concat([results_df_6, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.05769485245467778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.10608451692091882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.05425239156073909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.10243039389142378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.06666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.10295805739514349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.04350021884785024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.03685105463396325\n",
      "                Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  LogisticRegression   RandomOverSampler               0.05  0.057695\n",
      "1  LogisticRegression   RandomOverSampler               0.10  0.106085\n",
      "2  LogisticRegression               SMOTE               0.05  0.054252\n",
      "3  LogisticRegression               SMOTE               0.10  0.102430\n",
      "4  LogisticRegression  RandomUnderSampler               0.05  0.066667\n",
      "5  LogisticRegression  RandomUnderSampler               0.10  0.102958\n",
      "6  LogisticRegression            NearMiss               0.05  0.043500\n",
      "7  LogisticRegression            NearMiss               0.10  0.036851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    LogisticRegression(max_iter= 1000) \n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_6 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle de régression logistique\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_6 = pd.concat([results_df_6, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_6)\n",
    "results_df_6.to_pickle('dataframe_recap_sampling_6_1000.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réunit les résultats dans un seul tableau recapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_1 = pd.read_pickle('dataframe_recap_sampling_1.pkl')\n",
    "results_df_2 = pd.read_pickle('dataframe_recap_sampling_2.pkl')\n",
    "#results_df_3 = pd.read_pickle('dataframe_recap_sampling_3.pkl')\n",
    "results_df_4 = pd.read_pickle('dataframe_recap_sampling_4.pkl')\n",
    "results_df_5 = pd.read_pickle('dataframe_recap_sampling_5.pkl')\n",
    "results_df_6 = pd.read_pickle('dataframe_recap_sampling_6_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= pd.concat([results_df_1, results_df_2, results_df_4, results_df_5, results_df_6], ignore_index=True)\n",
    "results.to_pickle('dataframe_recap_sampling1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sampling = pd.read_pickle('dataframe_recap_sampling1.pkl')\n",
    "result_no_sampling = pd.read_pickle('dataframe_recap_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([result_sampling, result_no_sampling])\n",
    "results_df.to_pickle('dataframe_recap_sampling_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49</td>\n",
       "      <td>40</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.053933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>0.032535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.030263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model            Sampler  Sampling Strategy  \\\n",
       "count                       49                 40          40.000000   \n",
       "unique                       5                  4                NaN   \n",
       "top     RandomForestClassifier  RandomOverSampler                NaN   \n",
       "freq                        10                 10                NaN   \n",
       "mean                       NaN                NaN           0.075000   \n",
       "std                        NaN                NaN           0.025318   \n",
       "min                        NaN                NaN           0.050000   \n",
       "25%                        NaN                NaN           0.050000   \n",
       "50%                        NaN                NaN           0.075000   \n",
       "75%                        NaN                NaN           0.100000   \n",
       "max                        NaN                NaN           0.100000   \n",
       "\n",
       "         F1 Score  \n",
       "count   49.000000  \n",
       "unique        NaN  \n",
       "top           NaN  \n",
       "freq          NaN  \n",
       "mean     0.053933  \n",
       "std      0.032535  \n",
       "min      0.007539  \n",
       "25%      0.030263  \n",
       "50%      0.043478  \n",
       "75%      0.066667  \n",
       "max      0.131428  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model             Sampler  Sampling Strategy  F1 Score\n",
      "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
      "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
      "2      LogisticRegression   RandomOverSampler               0.10  0.106085\n",
      "3  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
      "4           XGBClassifier               SMOTE               0.10  0.077850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_27400\\346220120.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  best_results_df = pd.concat([best_results_df, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "# Grouper les résultats par modèle\n",
    "grouped_results = results_df.groupby('Model')\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les meilleurs résultats par modèle\n",
    "best_results_df = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque groupe\n",
    "for model, group_df in grouped_results:\n",
    "    # Trouver la ligne avec le plus grand F1 score pour chaque modèle\n",
    "    best_result = group_df.sort_values(by='F1 Score', ascending=False).iloc[0]\n",
    "    \n",
    "    # Ajouter le meilleur résultat au DataFrame final\n",
    "    # Remplacez la ligne best_results_df = best_results_df.append(...) par ceci\n",
    "    best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "        'Model': [best_result['Model']],\n",
    "        'Sampler': [best_result['Sampler']],\n",
    "        'Sampling Strategy': [best_result['Sampling Strategy']],\n",
    "        'F1 Score': [best_result['F1 Score']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Afficher le tableau récapitulatif des meilleurs résultats pour chaque modèle\n",
    "print(best_results_df)\n",
    "best_results_df.to_pickle('best_results_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.037149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.106085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomUnderSampler</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.102525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model             Sampler  Sampling Strategy  F1 Score\n",
       "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
       "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
       "2      LogisticRegression   RandomOverSampler               0.10  0.106085\n",
       "3  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
       "4           XGBClassifier               SMOTE               0.10  0.077850"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant qu'on a une vue d'ensemble claire sur les méthodes de re-échantillonage qui marchent le mieux en fonction des modèles, on va reprendre ces combinaisons pour réaliser une optimisation cette fois-ci des hyper-paramètres des modèles à l'aide d'un greedsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: GradientBoostingClassifier - Sampler: RandomUnderSampler - Sampling Strategy: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 0it [00:00, ?it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "Grid Search Progress: 1it [09:01, 541.09s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "Grid Search Progress: 2it [11:54:40, 25128.43s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "Grid Search Progress: 3it [13:34:31, 16389.93s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "Grid Search Progress: 4it [14:52:31, 11766.70s/it]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "Grid Search Progress: 5it [16:23:50, 11806.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres (Adaboost): {'classifier__estimator__max_depth': 2, 'classifier__learning_rate': 1, 'classifier__n_estimators': 50}\n",
      "Meilleur score F1 (Adaboost): 0.07971802099148495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'AdaBoostClassifier\n",
    "adab_classifier = AdaBoostClassifier(DecisionTreeClassifier()) # Utilise un arbre de décision faible par défaut\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', adab_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50],\n",
    "    'classifier__estimator__max_depth': [1, 2],  # Ajustez ici pour le paramètre de l'arbre faible\n",
    "    'classifier__learning_rate': [1],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "\n",
    "# Progress bar for grid search\n",
    "for train_index, test_index in tqdm(tscv.split(X_train, y_train), desc=\"Grid Search Progress\"):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (Adaboost):\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1 (Adaboost):\", grid_search.best_score_)\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_adaboost_newparam.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.05)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('classifier', rf_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, scoring='f1', cv=tscv)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (Random Forest):\", grid_search_rf.best_params_)\n",
    "print(\"Meilleur score F1 (Random Forest):\", grid_search_rf.best_score_)\n",
    "\n",
    "with open('modele_random_forest.pkl', 'wb') as file_rf:\n",
    "    pickle.dump(grid_search_rf, file_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du XGBClassifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('classifier', xgb_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [3, 7],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__learning_rate': [0.1, 0.01],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, scoring='f1', cv=tscv)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (XGBoost):\", grid_search_xgb.best_params_)\n",
    "print(\"Meilleur score F1 (XGBoost):\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Afficher les résultats détaillés de la recherche sur la grille\n",
    "results = grid_search_xgb.cv_results_\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    print(f\"Score moyen: {mean_score}, Paramètres: {params}\")\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_xgboost.pkl', 'wb') as file_xgb:\n",
    "    pickle.dump(grid_search_xgb, file_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.1)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    " \n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_gboost.pkl', 'wb') as file_gb:\n",
    "    pickle.dump(gb_classifier, file_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.1)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train_ros, y_train_ros)\n",
    "\n",
    " \n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compter le nombre d'éléments uniques dans y_train_rus\n",
    "unique_classes, counts = np.unique(y_train_rus, return_counts=True)\n",
    "\n",
    "# Afficher le nombre d'éléments uniques et leurs occurrences\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Classe {cls}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modele_regression_logistique.pkl', 'rb') as file:\n",
    "    modele_regression_logistique = pickle.load(file)\n",
    "\n",
    "with open('modele_xgboost.pkl', 'rb') as file:\n",
    "    modele_xgboost = pickle.load(file)\n",
    "\n",
    "with open('modele_adaboost.pkl', 'rb') as file:\n",
    "    modele_adaboost = pickle.load(file)\n",
    "\n",
    "with open('modele_arbre_decision.pkl', 'rb') as file:\n",
    "    modele_arbre_decision = pickle.load(file)\n",
    "\n",
    "modeles = [modele_regression_logistique, modele_xgboost, modele_adaboost]\n",
    "\n",
    "for model in modeles:\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer le F1-score sur l'ensemble de test\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Afficher le F1-score sur l'ensemble de test et le nom du modèle\n",
    "    print(f\"F1 Score sur l'ensemble de test ({model.__class__.__name__}): {f1_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score sur l'ensemble de test (GridSearchCV): 0.05073746312684366\n"
     ]
    }
   ],
   "source": [
    "with open('modele_adaboost_newparam.pkl', 'rb') as file:\n",
    "    modele_adaboost = pickle.load(file)\n",
    "    \n",
    "modeles = [modele_adaboost]\n",
    "\n",
    "for model in modeles:\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer le F1-score sur l'ensemble de test\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Afficher le F1-score sur l'ensemble de test et le nom du modèle\n",
    "    print(f\"F1 Score sur l'ensemble de test ({model.__class__.__name__}): {f1_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
