{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4646773 entries, 0 to 4646773\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   FlagImpaye       category      \n",
      " 1   Montant          float64       \n",
      " 2   DateTransaction  datetime64[ns]\n",
      " 3   CodeDecision     object        \n",
      " 4   VerifianceCPT1   int32         \n",
      " 5   VerifianceCPT2   int32         \n",
      " 6   VerifianceCPT3   int32         \n",
      " 7   D2CB             int32         \n",
      " 8   ScoringFP1       float64       \n",
      " 9   ScoringFP2       float64       \n",
      " 10  ScoringFP3       float64       \n",
      " 11  TauxImpNb_RB     float64       \n",
      " 12  TauxImpNB_CPM    float64       \n",
      " 13  EcartNumCheq     int32         \n",
      " 14  NbrMagasin3J     int32         \n",
      " 15  DiffDateTr1      float64       \n",
      " 16  DiffDateTr2      float64       \n",
      " 17  DiffDateTr3      float64       \n",
      " 18  CA3TRetMtt       float64       \n",
      " 19  CA3TR            float64       \n",
      " 20  Heure            int32         \n",
      " 21  Year             int32         \n",
      " 22  Month            int32         \n",
      " 23  Day              int32         \n",
      " 24  Hour             int32         \n",
      " 25  Minute           int32         \n",
      " 26  Second           int32         \n",
      "dtypes: category(1), datetime64[ns](1), float64(11), int32(13), object(1)\n",
      "memory usage: 731.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On définit la partie Train et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dates limites pour les ensembles d'apprentissage et de test\n",
    "train_inf = '2017-02-01'\n",
    "train_sup = '2017-08-31'\n",
    "test_inf = '2017-09-01'\n",
    "test_sup = '2017-11-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[(df['DateTransaction'] >= train_inf) & (df['DateTransaction'] <= train_sup)]\n",
    "X_train = train.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_train = train['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[(df['DateTransaction'] >= test_inf) & (df['DateTransaction'] <= test_sup)]\n",
    "X_test = test.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_test = test['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737068"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de chaque méthode d'échantillonage par modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'imblearn'\"\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install 'imblearn'\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\python311\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from xgboost) (1.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait tourner DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier,KNeighborsClassifier, SVC, adaboost et xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress: 100%|██████████| 4/4 [02:52<00:00, 43.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Définir les stratégies d'échantillonnage\n",
    "sampling_strategies = [0.05, 0.1]  # Ajoutez les valeurs que vous souhaitez tester\n",
    "\n",
    "#on garde deux oversampling et deux undersampling\n",
    "samplers = [RandomOverSampler, SMOTE, RandomUnderSampler, NearMiss]\n",
    "\n",
    "# Précalculer les échantillons resamplés pour chaque méthode d'échantillonnage\n",
    "resampled_data = {}\n",
    "for sampler in tqdm(samplers, desc=\"Resampling Progress\"):\n",
    "    for strategy in tqdm(sampling_strategies, desc=f\"{sampler.__name__} Progress\", leave=False):\n",
    "        key = (sampler, strategy)\n",
    "        X_resampled, y_resampled = sampler(sampling_strategy=strategy).fit_resample(X_train, y_train)\n",
    "        resampled_data[key] = (X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marevel\\AppData\\Local\\Temp\\ipykernel_14404\\260053566.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_1 = pd.concat([results_df_1, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.030195815287012748\n",
      "Model: DecisionTreeClassifier, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.026262189853104554\n",
      "Model: DecisionTreeClassifier, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.03714936856354632\n",
      "Model: DecisionTreeClassifier, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.03639382151402204\n",
      "Model: DecisionTreeClassifier, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.03486636000379733\n",
      "Model: DecisionTreeClassifier, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.03289138545840045\n",
      "Model: DecisionTreeClassifier, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.02929911735260891\n",
      "Model: DecisionTreeClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.025979530864366336\n",
      "                    Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  DecisionTreeClassifier   RandomOverSampler               0.05  0.030196\n",
      "1  DecisionTreeClassifier   RandomOverSampler               0.10  0.026262\n",
      "2  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
      "3  DecisionTreeClassifier               SMOTE               0.10  0.036394\n",
      "4  DecisionTreeClassifier  RandomUnderSampler               0.05  0.034866\n",
      "5  DecisionTreeClassifier  RandomUnderSampler               0.10  0.032891\n",
      "6  DecisionTreeClassifier            NearMiss               0.05  0.029299\n",
      "7  DecisionTreeClassifier            NearMiss               0.10  0.025980\n"
     ]
    }
   ],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_1 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_1 = pd.concat([results_df_1, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_1)\n",
    "results_df_1.to_pickle('dataframe_recap_sampling_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.02952237759137063\n",
      "                    Model   Sampler  Sampling Strategy  F1 Score\n",
      "0  RandomForestClassifier  NearMiss                0.1  0.029522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marevel\\AppData\\Local\\Temp\\ipykernel_14404\\4242767768.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_2_bis = pd.concat([results_df_2_bis, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_2 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_2 = pd.concat([results_df_2, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_2)\n",
    "results_df_2.to_pickle('dataframe_recap_sampling_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_3 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_3 = pd.concat([results_df_3, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                \n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_3)\n",
    "results_df_3.to_pickle('dataframe_recap_sampling_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_4 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_4 = pd.concat([results_df_4, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_4)\n",
    "results_df_4.to_pickle('dataframe_recap_sampling_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_5 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_5 = pd.concat([results_df_5, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_5)\n",
    "results_df_5.to_pickle('dataframe_recap_sampling_5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réunit les résultats dans un seul tableau recapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_1 = pd.read_pickle('dataframe_recap_sampling_1.pkl')\n",
    "results_df_2 = pd.read_pickle('dataframe_recap_sampling_2.pkl')\n",
    "#results_df_3 = pd.read_pickle('dataframe_recap_sampling_3.pkl')\n",
    "results_df_4 = pd.read_pickle('dataframe_recap_sampling_4.pkl')\n",
    "results_df_5 = pd.read_pickle('dataframe_recap_sampling_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df = pd.concat([results_df_1, results_df_2, results_df_3, results_df_4, results_df_5], ignore_index=True)\n",
    "results_df = pd.concat([results_df_1, results_df_2, results_df_4, results_df_5], ignore_index=True)\n",
    "results_df.to_pickle('dataframe_recap_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.056036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.032969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.015576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.031354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.038366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.072764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model            Sampler  Sampling Strategy  \\\n",
       "count                       32                 32            32.0000   \n",
       "unique                       4                  4                NaN   \n",
       "top     DecisionTreeClassifier  RandomOverSampler                NaN   \n",
       "freq                         8                  8                NaN   \n",
       "mean                       NaN                NaN             0.0750   \n",
       "std                        NaN                NaN             0.0254   \n",
       "min                        NaN                NaN             0.0500   \n",
       "25%                        NaN                NaN             0.0500   \n",
       "50%                        NaN                NaN             0.0750   \n",
       "75%                        NaN                NaN             0.1000   \n",
       "max                        NaN                NaN             0.1000   \n",
       "\n",
       "         F1 Score  \n",
       "count   32.000000  \n",
       "unique        NaN  \n",
       "top           NaN  \n",
       "freq          NaN  \n",
       "mean     0.056036  \n",
       "std      0.032969  \n",
       "min      0.015576  \n",
       "25%      0.031354  \n",
       "50%      0.038366  \n",
       "75%      0.072764  \n",
       "max      0.131428  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model             Sampler  Sampling Strategy  F1 Score\n",
      "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
      "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
      "2  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
      "3           XGBClassifier               SMOTE               0.10  0.077850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marevel\\AppData\\Local\\Temp\\ipykernel_16984\\346220120.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  best_results_df = pd.concat([best_results_df, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "# Grouper les résultats par modèle\n",
    "grouped_results = results_df.groupby('Model')\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les meilleurs résultats par modèle\n",
    "best_results_df = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque groupe\n",
    "for model, group_df in grouped_results:\n",
    "    # Trouver la ligne avec le plus grand F1 score pour chaque modèle\n",
    "    best_result = group_df.sort_values(by='F1 Score', ascending=False).iloc[0]\n",
    "    \n",
    "    # Ajouter le meilleur résultat au DataFrame final\n",
    "    # Remplacez la ligne best_results_df = best_results_df.append(...) par ceci\n",
    "    best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "        'Model': [best_result['Model']],\n",
    "        'Sampler': [best_result['Sampler']],\n",
    "        'Sampling Strategy': [best_result['Sampling Strategy']],\n",
    "        'F1 Score': [best_result['F1 Score']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Afficher le tableau récapitulatif des meilleurs résultats pour chaque modèle\n",
    "print(best_results_df)\n",
    "best_results_df.to_pickle('best_results_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.037149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomUnderSampler</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.102525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model             Sampler  Sampling Strategy  F1 Score\n",
       "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
       "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
       "2  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
       "3           XGBClassifier               SMOTE               0.10  0.077850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant qu'on a une vue d'ensemble claire sur les méthodes de re-échantillonage qui marchent le mieux en fonction des modèles, on va reprendre ces combinaisons pour réaliser une optimisation cette fois-ci des hyper-paramètres des modèles à l'aide d'un greedsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: GradientBoostingClassifier - Sampler: RandomUnderSampler - Sampling Strategy: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'AdaBoostClassifier\n",
    "adab_classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())  # Utilise un arbre de décision faible par défaut\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', adab_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__base_estimator__max_depth': [1, 2, 3],  # Ajustez ici pour le paramètre de l'arbre faible\n",
    "    'classifier__learning_rate': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)\n",
    "\n",
    "with open('modele_adaboost.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sous-échantillonnage\n",
    "smote = SMOTE(sampling_strategy=0.10)\n",
    "X_train_rus, y_train_rus = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('classifier', rf_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, scoring='f1', cv=tscv)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (Random Forest):\", grid_search_rf.best_params_)\n",
    "print(\"Meilleur score F1 (Random Forest):\", grid_search_rf.best_score_)\n",
    "\n",
    "with open('modele_random_forest.pkl', 'wb') as file_rf:\n",
    "    pickle.dump(grid_search_rf, file_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du XGBClassifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('classifier', xgb_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [3, 5, 7],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, scoring='f1', cv=tscv)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (XGBoost):\", grid_search_xgb.best_params_)\n",
    "print(\"Meilleur score F1 (XGBoost):\", grid_search_xgb.best_score_)\n",
    "\n",
    "with open('modele_xgboost.pkl', 'wb') as file_xgb:\n",
    "    pickle.dump(grid_search_xgb, file_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres: {'classifier__max_depth': 15, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n",
      "Meilleur score F1: 0.10323029387708163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.1)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    " \n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.1)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train_ros, y_train_ros)\n",
    "\n",
    " \n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 0: 2290 occurrences\n",
      "Classe 1: 229 occurrences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compter le nombre d'éléments uniques dans y_train_rus\n",
    "unique_classes, counts = np.unique(y_train_rus, return_counts=True)\n",
    "\n",
    "# Afficher le nombre d'éléments uniques et leurs occurrences\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Classe {cls}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38827"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score sur l'ensemble de test: 0.054590570719602965\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Utiliser le meilleur modèle trouvé par la recherche sur la grille\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculer le F1-score sur l'ensemble de test\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "# Afficher le F1-score sur l'ensemble de test\n",
    "print(\"F1 Score sur l'ensemble de test:\", f1_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
