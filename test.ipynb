{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframe.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On définit la partie Train et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dates limites pour les ensembles d'apprentissage et de test\n",
    "train_inf = '2017-02-01'\n",
    "train_sup = '2017-08-31'\n",
    "test_inf = '2017-09-01'\n",
    "test_sup = '2017-11-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[(df['DateTransaction'] >= train_inf) & (df['DateTransaction'] <= train_sup)]\n",
    "X_train = train.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_train = train['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[(df['DateTransaction'] >= test_inf) & (df['DateTransaction'] <= test_sup)]\n",
    "X_test = test.drop(columns=['FlagImpaye','CodeDecision','DateTransaction'])\n",
    "y_test = test['FlagImpaye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3888468"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de chaque méthode d'échantillonage par modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'imblearn'\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\python311\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from xgboost) (1.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait tourner DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier,KNeighborsClassifier, SVC, adaboost et xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress: 100%|██████████| 4/4 [14:03<00:00, 210.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Définir les stratégies d'échantillonnage\n",
    "sampling_strategies = [0.05, 0.1]  # Ajoutez les valeurs que vous souhaitez tester\n",
    "#on garde deux oversampling et deux undersampling\n",
    "samplers = [RandomOverSampler, SMOTE, RandomUnderSampler, NearMiss]\n",
    "\n",
    "# Précalculer les échantillons resamplés pour chaque méthode d'échantillonnage\n",
    "resampled_data = {}\n",
    "for sampler in tqdm(samplers, desc=\"Resampling Progress\"):\n",
    "    for strategy in tqdm(sampling_strategies, desc=f\"{sampler.__name__} Progress\", leave=False):\n",
    "        key = (sampler, strategy)\n",
    "        X_resampled, y_resampled = sampler(sampling_strategy=strategy).fit_resample(X_train, y_train)\n",
    "        resampled_data[key] = (X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling Progress:  75%|███████▌  | 3/4 [01:43<00:34, 34.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m strategy \u001b[38;5;129;01min\u001b[39;00m tqdm(sampling_strategies, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampler\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m         key \u001b[38;5;241m=\u001b[39m (sampler, strategy)\n\u001b[1;32m---> 22\u001b[0m         X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         resampled_data[key] \u001b[38;5;241m=\u001b[39m (X_resampled, y_resampled)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Définir les modèles à tester\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:244\u001b[0m, in \u001b[0;36mNearMiss._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    241\u001b[0m y_class \u001b[38;5;241m=\u001b[39m _safe_indexing(y, target_class_indices)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m     dist_vec, idx_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     index_target_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection_dist_based(\n\u001b[0;32m    248\u001b[0m         X,\n\u001b[0;32m    249\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         sel_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    254\u001b[0m     )\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:822\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    815\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    818\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 822\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    834\u001b[0m ):\n\u001b[0;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    836\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    837\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:259\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    272\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    273\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:90\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\threadpoolctl.py:440\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_1 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_1 = pd.concat([results_df_1, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_1)\n",
    "results_df_1.to_pickle('dataframe_recap_sampling_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_2 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_2 = pd.concat([results_df_2, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_2)\n",
    "results_df_2.to_pickle('dataframe_recap_sampling_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_3 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_3 = pd.concat([results_df_3, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_3)\n",
    "results_df_3.to_pickle('dataframe_recap_sampling_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4044\\3750963144.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_4 = pd.concat([results_df_4, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.11692036179130819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.13142780820088154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.015575857420997454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.022629149918118208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.10746336476201294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.12748484650980904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.03958164977932492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.03174043378592841\n",
      "                Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  AdaBoostClassifier   RandomOverSampler               0.05  0.116920\n",
      "1  AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
      "2  AdaBoostClassifier               SMOTE               0.05  0.015576\n",
      "3  AdaBoostClassifier               SMOTE               0.10  0.022629\n",
      "4  AdaBoostClassifier  RandomUnderSampler               0.05  0.107463\n",
      "5  AdaBoostClassifier  RandomUnderSampler               0.10  0.127485\n",
      "6  AdaBoostClassifier            NearMiss               0.05  0.039582\n",
      "7  AdaBoostClassifier            NearMiss               0.10  0.031740\n"
     ]
    }
   ],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_4 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_4 = pd.concat([results_df_4, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_4)\n",
    "results_df_4.to_pickle('dataframe_recap_sampling_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4044\\3536015111.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_5 = pd.concat([results_df_5, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBClassifier, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.0715716151501959\n",
      "Model: XGBClassifier, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.058997050147492625\n",
      "Model: XGBClassifier, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.06164853474468294\n",
      "Model: XGBClassifier, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.07784960871044573\n",
      "Model: XGBClassifier, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.07633948494474051\n",
      "Model: XGBClassifier, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.0656522313176657\n",
      "Model: XGBClassifier, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.03485663082437276\n",
      "Model: XGBClassifier, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.029324231670471584\n",
      "           Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  XGBClassifier   RandomOverSampler               0.05  0.071572\n",
      "1  XGBClassifier   RandomOverSampler               0.10  0.058997\n",
      "2  XGBClassifier               SMOTE               0.05  0.061649\n",
      "3  XGBClassifier               SMOTE               0.10  0.077850\n",
      "4  XGBClassifier  RandomUnderSampler               0.05  0.076339\n",
      "5  XGBClassifier  RandomUnderSampler               0.10  0.065652\n",
      "6  XGBClassifier            NearMiss               0.05  0.034857\n",
      "7  XGBClassifier            NearMiss               0.10  0.029324\n"
     ]
    }
   ],
   "source": [
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_5 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_5 = pd.concat([results_df_5, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_5)\n",
    "results_df_5.to_pickle('dataframe_recap_sampling_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24944\\664018385.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df_6 = pd.concat([results_df_6, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomOverSampler, Strategy: 0.05, F1 Score: 0.05769485245467778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomOverSampler, Strategy: 0.1, F1 Score: 0.10608451692091882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: SMOTE, Strategy: 0.05, F1 Score: 0.05425239156073909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: SMOTE, Strategy: 0.1, F1 Score: 0.10243039389142378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomUnderSampler, Strategy: 0.05, F1 Score: 0.06666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: RandomUnderSampler, Strategy: 0.1, F1 Score: 0.10295805739514349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: NearMiss, Strategy: 0.05, F1 Score: 0.04350021884785024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression, Sampler: NearMiss, Strategy: 0.1, F1 Score: 0.03685105463396325\n",
      "                Model             Sampler  Sampling Strategy  F1 Score\n",
      "0  LogisticRegression   RandomOverSampler               0.05  0.057695\n",
      "1  LogisticRegression   RandomOverSampler               0.10  0.106085\n",
      "2  LogisticRegression               SMOTE               0.05  0.054252\n",
      "3  LogisticRegression               SMOTE               0.10  0.102430\n",
      "4  LogisticRegression  RandomUnderSampler               0.05  0.066667\n",
      "5  LogisticRegression  RandomUnderSampler               0.10  0.102958\n",
      "6  LogisticRegression            NearMiss               0.05  0.043500\n",
      "7  LogisticRegression            NearMiss               0.10  0.036851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Définir les modèles à tester\n",
    "models = [\n",
    "    LogisticRegression(max_iter= 1000) \n",
    "]\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats\n",
    "results_df_6 = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque modèle\n",
    "for model in models:\n",
    "    # Parcourir chaque échantillonneur\n",
    "    for sampler in samplers:\n",
    "        # Parcourir chaque valeur de sampling_strategy\n",
    "        for strategy in sampling_strategies:\n",
    "            # Récupérer les échantillons resamplés précalculés\n",
    "            X_resampled, y_resampled = resampled_data[(sampler, strategy)]\n",
    "\n",
    "            # Initialiser et entraîner le modèle de régression logistique\n",
    "            model.fit(X_resampled, y_resampled)\n",
    "\n",
    "            # Faire des prédictions sur l'ensemble de test\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Évaluer les performances du modèle\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Ajouter les résultats au DataFrame\n",
    "            results_df_6 = pd.concat([results_df_6, pd.DataFrame({\n",
    "                'Model': [model.__class__.__name__],\n",
    "                'Sampler': [sampler.__name__],\n",
    "                'Sampling Strategy': [strategy],\n",
    "                'F1 Score': [f1]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Afficher le message de progression\n",
    "            tqdm.write(f\"Model: {model.__class__.__name__}, Sampler: {sampler.__name__}, Strategy: {strategy}, F1 Score: {f1}\")\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df_6)\n",
    "results_df_6.to_pickle('dataframe_recap_sampling_6_1000.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réunit les résultats dans un seul tableau recapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_1 = pd.read_pickle('dataframe_recap_sampling_1.pkl')\n",
    "results_df_2 = pd.read_pickle('dataframe_recap_sampling_2.pkl')\n",
    "#results_df_3 = pd.read_pickle('dataframe_recap_sampling_3.pkl')\n",
    "results_df_4 = pd.read_pickle('dataframe_recap_sampling_4.pkl')\n",
    "results_df_5 = pd.read_pickle('dataframe_recap_sampling_5.pkl')\n",
    "results_df_6 = pd.read_pickle('dataframe_recap_sampling_6_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= pd.concat([results_df_1, results_df_2, results_df_4, results_df_5, results_df_6], ignore_index=True)\n",
    "results.to_pickle('dataframe_recap_sampling1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sampling = pd.read_pickle('dataframe_recap_sampling1.pkl')\n",
    "result_no_sampling = pd.read_pickle('dataframe_recap_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([result_sampling, result_no_sampling])\n",
    "results_df.to_pickle('dataframe_recap_sampling_no_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>0.032489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.031740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.071572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model            Sampler  Sampling Strategy  \\\n",
       "count                       45                 40          40.000000   \n",
       "unique                       5                  4                NaN   \n",
       "top     DecisionTreeClassifier  RandomOverSampler                NaN   \n",
       "freq                         9                 10                NaN   \n",
       "mean                       NaN                NaN           0.075000   \n",
       "std                        NaN                NaN           0.025318   \n",
       "min                        NaN                NaN           0.050000   \n",
       "25%                        NaN                NaN           0.050000   \n",
       "50%                        NaN                NaN           0.075000   \n",
       "75%                        NaN                NaN           0.100000   \n",
       "max                        NaN                NaN           0.100000   \n",
       "\n",
       "         F1 Score  \n",
       "count   45.000000  \n",
       "unique        NaN  \n",
       "top           NaN  \n",
       "freq          NaN  \n",
       "mean     0.055962  \n",
       "std      0.032489  \n",
       "min      0.007539  \n",
       "25%      0.031740  \n",
       "50%      0.043478  \n",
       "75%      0.071572  \n",
       "max      0.131428  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model             Sampler  Sampling Strategy  F1 Score\n",
      "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
      "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
      "2      LogisticRegression   RandomOverSampler               0.10  0.106085\n",
      "3  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
      "4           XGBClassifier               SMOTE               0.10  0.077850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_28352\\346220120.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  best_results_df = pd.concat([best_results_df, pd.DataFrame({\n"
     ]
    }
   ],
   "source": [
    "# Grouper les résultats par modèle\n",
    "grouped_results = results_df.groupby('Model')\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les meilleurs résultats par modèle\n",
    "best_results_df = pd.DataFrame(columns=['Model', 'Sampler', 'Sampling Strategy', 'F1 Score'])\n",
    "\n",
    "# Parcourir chaque groupe\n",
    "for model, group_df in grouped_results:\n",
    "    # Trouver la ligne avec le plus grand F1 score pour chaque modèle\n",
    "    best_result = group_df.sort_values(by='F1 Score', ascending=False).iloc[0]\n",
    "    \n",
    "    # Ajouter le meilleur résultat au DataFrame final\n",
    "    # Remplacez la ligne best_results_df = best_results_df.append(...) par ceci\n",
    "    best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "        'Model': [best_result['Model']],\n",
    "        'Sampler': [best_result['Sampler']],\n",
    "        'Sampling Strategy': [best_result['Sampling Strategy']],\n",
    "        'F1 Score': [best_result['F1 Score']]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Afficher le tableau récapitulatif des meilleurs résultats pour chaque modèle\n",
    "print(best_results_df)\n",
    "best_results_df.to_pickle('best_results_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampler</th>\n",
       "      <th>Sampling Strategy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.131428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.037149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.106085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomUnderSampler</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.102525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model             Sampler  Sampling Strategy  F1 Score\n",
       "0      AdaBoostClassifier   RandomOverSampler               0.10  0.131428\n",
       "1  DecisionTreeClassifier               SMOTE               0.05  0.037149\n",
       "2      LogisticRegression   RandomOverSampler               0.10  0.106085\n",
       "3  RandomForestClassifier  RandomUnderSampler               0.05  0.102525\n",
       "4           XGBClassifier               SMOTE               0.10  0.077850"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant qu'on a une vue d'ensemble claire sur les méthodes de re-échantillonage qui marchent le mieux en fonction des modèles, on va reprendre ces combinaisons pour réaliser une optimisation cette fois-ci des hyper-paramètres des modèles à l'aide d'un greedsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: GradientBoostingClassifier - Sampler: RandomUnderSampler - Sampling Strategy: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 0it [00:00, ?it/s]C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'AdaBoostClassifier\n",
    "adab_classifier = AdaBoostClassifier(DecisionTreeClassifier()) # Utilise un arbre de décision faible par défaut\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', adab_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__estimator__max_depth': [2, 3],  # Ajustez ici pour le paramètre de l'arbre faible\n",
    "    #'classifier__learning_rate': [0.1, 0.5],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "\n",
    "# Progress bar for grid search\n",
    "for train_index, test_index in tqdm(tscv.split(X_train, y_train), desc=\"Grid Search Progress\"):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (Adaboost):\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1 (Adaboost):\", grid_search.best_score_)\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_adaboost.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.05)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('classifier', rf_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, scoring='f1', cv=tscv)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (Random Forest):\", grid_search_rf.best_params_)\n",
    "print(\"Meilleur score F1 (Random Forest):\", grid_search_rf.best_score_)\n",
    "\n",
    "with open('modele_random_forest.pkl', 'wb') as file_rf:\n",
    "    pickle.dump(grid_search_rf, file_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.10)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle du XGBClassifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('classifier', xgb_classifier)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [3, 7],  # Ajustez ici pour le paramètre de profondeur\n",
    "    'classifier__learning_rate': [0.1, 0.01],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, scoring='f1', cv=tscv)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres (XGBoost):\", grid_search_xgb.best_params_)\n",
    "print(\"Meilleur score F1 (XGBoost):\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Afficher les résultats détaillés de la recherche sur la grille\n",
    "results = grid_search_xgb.cv_results_\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"]):\n",
    "    print(f\"Score moyen: {mean_score}, Paramètres: {params}\")\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_xgboost.pkl', 'wb') as file_xgb:\n",
    "    pickle.dump(grid_search_xgb, file_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "rus = RandomUnderSampler(sampling_strategy=0.1)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    " \n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)\n",
    "\n",
    "# Enregistrement du modèle\n",
    "with open('modele_gboost.pkl', 'wb') as file_gb:\n",
    "    pickle.dump(gb_classifier, file_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sous-échantillonnage\n",
    "ros = RandomOverSampler(sampling_strategy=0.1)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Modèle de l'arbre de décision\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', gb_classifier)\n",
    "])\n",
    "\n",
    "# Créer un objet TimeSeriesSplit pour la cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "# Paramètres pour la recherche sur la grille\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "# Recherche sur la grille\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=tscv)\n",
    "grid_search.fit(X_train_ros, y_train_ros)\n",
    "\n",
    " \n",
    "# Afficher les meilleurs paramètres et le meilleur score F1\n",
    "print(\"Meilleurs paramètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score F1:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compter le nombre d'éléments uniques dans y_train_rus\n",
    "unique_classes, counts = np.unique(y_train_rus, return_counts=True)\n",
    "\n",
    "# Afficher le nombre d'éléments uniques et leurs occurrences\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Classe {cls}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser le meilleur modèle trouvé par la recherche sur la grille\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculer le F1-score sur l'ensemble de test\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "# Afficher le F1-score sur l'ensemble de test\n",
    "print(\"F1 Score sur l'ensemble de test:\", f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
